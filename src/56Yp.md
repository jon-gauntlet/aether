# Security Findings ETL Pipeline

This ETL (Extract, Transform, Load) pipeline processes security findings from various sources (SAST, DAST, LLM) and normalizes them into a consistent format for further analysis.

## Overview

The pipeline supports the following sources:

### SAST (Static Application Security Testing)
- Bearer
- Brakeman

### DAST (Dynamic Application Security Testing)
- Nikto
- Wapiti
- OWASP ZAP

### LLM (Large Language Model)
- Claude 3.5 Sonnet

## Directory Structure

```
scripts/etl/
├── config/
│   └── parser_configs.yaml    # Parser configurations
├── parsers/
│   ├── base_parser.py         # Base parser class
│   ├── sast/
│   │   ├── bearer_parser.py   # Bearer parser
│   │   └── brakeman_parser.py # Brakeman parser
│   ├── dast/
│   │   ├── nikto_parser.py    # Nikto parser
│   │   ├── wapiti_parser.py   # Wapiti parser
│   │   └── zap_parser.py      # ZAP parser
│   └── llm/
│       └── claude_parser.py   # Claude parser
└── main.py                    # Main ETL runner script
```

## Usage

Run the ETL pipeline using the following command:

```bash
python -m scripts.etl.main [options]
```

### Options

- `--config PATH`: Path to parser configs YAML (default: scripts/etl/config/parser_configs.yaml)
- `--schema PATH`: Path to normalized schema JSON (default: data/schemas/normalized_finding.json)
- `--source-types TYPE [TYPE ...]`: Source types to process (default: all)
- `--dry-run`: Validate and print findings without saving
- `--log-level {DEBUG,INFO,WARNING,ERROR}`: Logging level (default: INFO)

### Examples

Process all sources:
```bash
python -m scripts.etl.main
```

Process only SAST findings:
```bash
python -m scripts.etl.main --source-types bearer brakeman
```

Dry run with debug logging:
```bash
python -m scripts.etl.main --dry-run --log-level DEBUG
```

## Configuration

The pipeline is configured using a YAML file (`parser_configs.yaml`) that defines:

- Source directories and file patterns
- Required fields for each source
- Fields to preserve in normalized output
- Error handling behavior

### Example Configuration

```yaml
bearer:
  source_dir: findings/sast/bearer
  file_pattern: "*.json"
  required_fields:
    - id
    - title
    - description
    - severity
  preserve_fields:
    - rule_id
    - categories
    - tags
```

## Output Format

The normalized findings are saved in JSON format with the following structure:

```json
{
  "id": "TOOL-XXXX",
  "title": "Finding Title",
  "source": {
    "tool": "tool_name",
    "type": "sast|dast|llm",
    "original_id": "original_id"
  },
  "description": "Finding description",
  "finding_type": "vulnerable_code|misconfiguration|...",
  "security_category": "category",
  "severity": {
    "level": "high|medium|low",
    "score": 7.0,
    "justification": "Reason for severity"
  },
  "evidence": {
    "file": "path/to/file",
    "line": 123,
    "code": "vulnerable code",
    "reproduction": [
      "Step 1",
      "Step 2"
    ]
  },
  "remediation": [
    "Step 1",
    "Step 2"
  ],
  "references": [
    {
      "title": "Reference Title",
      "url": "https://..."
    }
  ],
  "metadata": {
    "cwe_id": ["CWE-XXX"],
    "tags": ["tag1", "tag2"]
  }
}
```

## Development

### Adding a New Parser

1. Create a new parser class in the appropriate directory (sast/dast/llm)
2. Inherit from `BaseParser` and implement the `parse` method
3. Add parser configuration to `parser_configs.yaml`
4. Update `get_parser_class` in `main.py`

### Parser Requirements

Each parser must:

1. Inherit from `BaseParser`
2. Implement the `parse` method
3. Handle required fields validation
4. Normalize severity levels
5. Preserve specified source fields
6. Validate output against schema

### Error Handling

The pipeline provides several error handling mechanisms:

- Skip on error option per parser
- Detailed error logging
- Validation at multiple stages
- Parallel processing with proper error propagation

## Contributing

When contributing to this project:

1. Follow the existing code style
2. Add appropriate tests
3. Update documentation
4. Test thoroughly with sample data

## License

This project is licensed under the MIT License - see the LICENSE file for details. 