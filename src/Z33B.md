# Security Findings ETL Pipeline

## Overview

This ETL pipeline normalizes security findings from multiple sources into a consistent format while preserving all source data. It handles:

- SAST findings (Brakeman, Bearer, Snyk, etc.)
- DAST findings (Wapiti, Nikto, ZAP, etc.) 
- LLM-based findings (Claude 3.5 Sonnet)

## Design Principles

1. **Data Preservation**
   - All source data is preserved in the normalized output
   - No information loss during transformation
   - Full traceability back to source

2. **Robustness**
   - Comprehensive error handling
   - Detailed logging
   - Schema validation at multiple stages
   - Progress reporting
   - Dry-run capability

3. **Extensibility**
   - Modular parser architecture
   - Configuration-driven transformations
   - Easy to add new data sources

## Usage

```bash
# Run the full ETL pipeline
python -m scripts.etl.main

# Run with dry-run mode
python -m scripts.etl.main --dry-run

# Run for specific sources
python -m scripts.etl.main --sources sast dast

# Run with detailed logging
python -m scripts.etl.main --log-level DEBUG
```

## Directory Structure

```
etl/
  parsers/           # Source-specific parsers
  transformers/      # Data transformation logic
  loaders/           # Output writers
  utils/            # Shared utilities
  config/           # Configuration files
  main.py           # Main entry point
```

## Adding New Sources

1. Create a new parser in `parsers/`
2. Add parser configuration in `config/parser_configs.yaml`
3. Add field mappings in `config/field_mappings.yaml`
4. Update tests

## Output

Normalized findings are written to:
`/home/jon/security-assessment/data/normalized_findings/`

Each finding preserves its source data while conforming to the normalized schema. 